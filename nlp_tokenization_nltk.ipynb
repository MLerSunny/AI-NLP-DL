{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS9yacW/vwS3/0cdRzx5ew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLerSunny/AI-NLP-DL/blob/main/nlp_tokenization_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: install nktk\n",
        "!pip install --upgrade --force-reinstall nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCSCOWiMGsvy",
        "outputId": "50075431-c567-4c42-8a0b-f5ce1371a093"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk\n",
            "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting click (from nltk)\n",
            "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
            "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "Successfully installed click-8.1.8 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"Roadmap To Learn NLP!. Practical Usecase's Of NLP!. Tokenization And Basic-Terminologies. Tokenization Practicals!!. Text Preprocessing Stemming Using NLTK. Text Preprocessing Lemmatization USing NLTK, Stopwords, Parts Of Speech, NAmed Entity Recognition, Different types Of Encoding, Word Embedding, Word2vec, Skipgram Indepth Intuition, Average Word2vec With Implementation.\"\"\""
      ],
      "metadata": {
        "id": "OEG8WUw9IvI0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on1qaQvWKXRP",
        "outputId": "061e45a6-6472-4309-bffa-189b86971bbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roadmap To Learn NLP!. Practical Usecase's Of NLP!. Tokenization And Basic-Terminologies. Tokenization Practicals!!. Text Preprocessing Stemming Using NLTK. Text Preprocessing Lemmatization USing NLTK, Stopwords, Parts Of Speech, NAmed Entity Recognition, Different types Of Encoding, Word Embedding, Word2vec, Skipgram Indepth Intuition, Average Word2vec With Implementation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization\n",
        "## Sentence--> Paragraph\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "KwXkoloTJrSK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swk5DY_FL0UR",
        "outputId": "af39ff99-f32a-43b4-fa82-97ea52516070"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Roadmap To Learn NLP!.', \"Practical Usecase's Of NLP!.\", 'Tokenization And Basic-Terminologies.', 'Tokenization Practicals!!.', 'Text Preprocessing Stemming Using NLTK.', 'Text Preprocessing Lemmatization USing NLTK, Stopwords, Parts Of Speech, NAmed Entity Recognition, Different types Of Encoding, Word Embedding, Word2vec, Skipgram Indepth Intuition, Average Word2vec With Implementation.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvpzIc7wMeve",
        "outputId": "3a5a79a9-9d29-446d-871c-00603a300211"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization Practicals!!.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "500aAQF-U-1_",
        "outputId": "30e9cded-0e33-4538-cd7a-26ff014085eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "words = word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "iJ_zH1AtVIm8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FwG9c-PVPJ9",
        "outputId": "0869b3f3-c5a9-4918-bbef-b99f683f86a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Roadmap', 'To', 'Learn', 'NLP', '!', '.', 'Practical', 'Usecase', \"'s\", 'Of', 'NLP', '!', '.', 'Tokenization', 'And', 'Basic-Terminologies', '.', 'Tokenization', 'Practicals', '!', '!', '.', 'Text', 'Preprocessing', 'Stemming', 'Using', 'NLTK', '.', 'Text', 'Preprocessing', 'Lemmatization', 'USing', 'NLTK', ',', 'Stopwords', ',', 'Parts', 'Of', 'Speech', ',', 'NAmed', 'Entity', 'Recognition', ',', 'Different', 'types', 'Of', 'Encoding', ',', 'Word', 'Embedding', ',', 'Word2vec', ',', 'Skipgram', 'Indepth', 'Intuition', ',', 'Average', 'Word2vec', 'With', 'Implementation', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for iw in words:\n",
        "  print(iw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luJzt70XWdo3",
        "outputId": "fafda6a5-e02c-4b94-cca2-ff0dce9d5583"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roadmap\n",
            "To\n",
            "Learn\n",
            "NLP\n",
            "!\n",
            ".\n",
            "Practical\n",
            "Usecase\n",
            "'s\n",
            "Of\n",
            "NLP\n",
            "!\n",
            ".\n",
            "Tokenization\n",
            "And\n",
            "Basic-Terminologies\n",
            ".\n",
            "Tokenization\n",
            "Practicals\n",
            "!\n",
            "!\n",
            ".\n",
            "Text\n",
            "Preprocessing\n",
            "Stemming\n",
            "Using\n",
            "NLTK\n",
            ".\n",
            "Text\n",
            "Preprocessing\n",
            "Lemmatization\n",
            "USing\n",
            "NLTK\n",
            ",\n",
            "Stopwords\n",
            ",\n",
            "Parts\n",
            "Of\n",
            "Speech\n",
            ",\n",
            "NAmed\n",
            "Entity\n",
            "Recognition\n",
            ",\n",
            "Different\n",
            "types\n",
            "Of\n",
            "Encoding\n",
            ",\n",
            "Word\n",
            "Embedding\n",
            ",\n",
            "Word2vec\n",
            ",\n",
            "Skipgram\n",
            "Indepth\n",
            "Intuition\n",
            ",\n",
            "Average\n",
            "Word2vec\n",
            "With\n",
            "Implementation\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "word_punct = wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "id": "QL6Ew5exVRTh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eFmEnSBVyX_",
        "outputId": "2f4f1df3-8d0a-4936-cf5e-69f85961fb9b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Roadmap', 'To', 'Learn', 'NLP', '!.', 'Practical', 'Usecase', \"'\", 's', 'Of', 'NLP', '!.', 'Tokenization', 'And', 'Basic', '-', 'Terminologies', '.', 'Tokenization', 'Practicals', '!!.', 'Text', 'Preprocessing', 'Stemming', 'Using', 'NLTK', '.', 'Text', 'Preprocessing', 'Lemmatization', 'USing', 'NLTK', ',', 'Stopwords', ',', 'Parts', 'Of', 'Speech', ',', 'NAmed', 'Entity', 'Recognition', ',', 'Different', 'types', 'Of', 'Encoding', ',', 'Word', 'Embedding', ',', 'Word2vec', ',', 'Skipgram', 'Indepth', 'Intuition', ',', 'Average', 'Word2vec', 'With', 'Implementation', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in word_punct:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-btFWpJWXPr",
        "outputId": "828b74b7-0079-4368-9ace-d7a92da17c11"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Roadmap\n",
            "To\n",
            "Learn\n",
            "NLP\n",
            "!.\n",
            "Practical\n",
            "Usecase\n",
            "'\n",
            "s\n",
            "Of\n",
            "NLP\n",
            "!.\n",
            "Tokenization\n",
            "And\n",
            "Basic\n",
            "-\n",
            "Terminologies\n",
            ".\n",
            "Tokenization\n",
            "Practicals\n",
            "!!.\n",
            "Text\n",
            "Preprocessing\n",
            "Stemming\n",
            "Using\n",
            "NLTK\n",
            ".\n",
            "Text\n",
            "Preprocessing\n",
            "Lemmatization\n",
            "USing\n",
            "NLTK\n",
            ",\n",
            "Stopwords\n",
            ",\n",
            "Parts\n",
            "Of\n",
            "Speech\n",
            ",\n",
            "NAmed\n",
            "Entity\n",
            "Recognition\n",
            ",\n",
            "Different\n",
            "types\n",
            "Of\n",
            "Encoding\n",
            ",\n",
            "Word\n",
            "Embedding\n",
            ",\n",
            "Word2vec\n",
            ",\n",
            "Skipgram\n",
            "Indepth\n",
            "Intuition\n",
            ",\n",
            "Average\n",
            "Word2vec\n",
            "With\n",
            "Implementation\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAScfWE4WqUz",
        "outputId": "5121f84b-bd93-47a6-d05b-4a3de1703637"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Roadmap',\n",
              " 'To',\n",
              " 'Learn',\n",
              " 'NLP',\n",
              " '!',\n",
              " '.',\n",
              " 'Practical',\n",
              " 'Usecase',\n",
              " \"'s\",\n",
              " 'Of',\n",
              " 'NLP',\n",
              " '!',\n",
              " '.',\n",
              " 'Tokenization',\n",
              " 'And',\n",
              " 'Basic-Terminologies.',\n",
              " 'Tokenization',\n",
              " 'Practicals',\n",
              " '!',\n",
              " '!',\n",
              " '.',\n",
              " 'Text',\n",
              " 'Preprocessing',\n",
              " 'Stemming',\n",
              " 'Using',\n",
              " 'NLTK.',\n",
              " 'Text',\n",
              " 'Preprocessing',\n",
              " 'Lemmatization',\n",
              " 'USing',\n",
              " 'NLTK',\n",
              " ',',\n",
              " 'Stopwords',\n",
              " ',',\n",
              " 'Parts',\n",
              " 'Of',\n",
              " 'Speech',\n",
              " ',',\n",
              " 'NAmed',\n",
              " 'Entity',\n",
              " 'Recognition',\n",
              " ',',\n",
              " 'Different',\n",
              " 'types',\n",
              " 'Of',\n",
              " 'Encoding',\n",
              " ',',\n",
              " 'Word',\n",
              " 'Embedding',\n",
              " ',',\n",
              " 'Word2vec',\n",
              " ',',\n",
              " 'Skipgram',\n",
              " 'Indepth',\n",
              " 'Intuition',\n",
              " ',',\n",
              " 'Average',\n",
              " 'Word2vec',\n",
              " 'With',\n",
              " 'Implementation',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}